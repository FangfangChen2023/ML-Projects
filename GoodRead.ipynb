{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tFD1ychEn1-G",
        "ff49kRzMknQ-",
        "9t8kSy_DlU79",
        "yCunOeBqPRzn",
        "HdiRscd_uysD",
        "g94LbGcIvHqt",
        "Bnog70StwSpj",
        "aqbjz_r4grEF",
        "kDS3z1v0kl4_",
        "0cT7BE9wkvTJ",
        "cfHUCGhKhBzJ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FangfangChen2023/ML-Projects/blob/main/GoodRead.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "tFD1ychEn1-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import pydot\n",
        "import graphviz\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "bXLweKVX8Q0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data"
      ],
      "metadata": {
        "id": "ff49kRzMknQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For loading the complete dataset\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c cs985-cs987-goodread-class-project"
      ],
      "metadata": {
        "id": "r9CmJBpEK-ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/cs985-cs987-goodread-class-project.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFg73nqrLf0x",
        "outputId": "05f2a315-971c-418d-fa62-ea24062894fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/cs985-cs987-goodread-class-project.zip\n",
            "  inflating: my_goodreads_sample_submission.csv  \n",
            "  inflating: my_goodreads_test.csv   \n",
            "  inflating: my_goodreads_train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# goodreads_train = pd.read_csv(\"my_goodreads_train.csv\", error_bad_lines=False, engine=\"python\")\n",
        "goodreads_train = pd.read_csv(\"my_goodreads_train.csv\")\n",
        "goodreads_train.info()\n",
        "# goodreads_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiCUchxDBWwD",
        "outputId": "07567352-2385-402e-e304-7a419df08ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 700000 entries, 0 to 699999\n",
            "Data columns (total 11 columns):\n",
            " #   Column        Non-Null Count   Dtype \n",
            "---  ------        --------------   ----- \n",
            " 0   user_id       700000 non-null  object\n",
            " 1   book_id       700000 non-null  int64 \n",
            " 2   review_id     700000 non-null  object\n",
            " 3   rating        700000 non-null  int64 \n",
            " 4   review_text   700000 non-null  object\n",
            " 5   date_added    700000 non-null  object\n",
            " 6   date_updated  700000 non-null  object\n",
            " 7   read_at       628588 non-null  object\n",
            " 8   started_at    486589 non-null  object\n",
            " 9   n_votes       700000 non-null  int64 \n",
            " 10  n_comments    700000 non-null  int64 \n",
            "dtypes: int64(4), object(7)\n",
            "memory usage: 58.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# goodreads_test = pd.read_csv(\"my_goodreads_test.csv\", error_bad_lines=False, engine=\"python\")\n",
        "goodreads_test = pd.read_csv(\"my_goodreads_test.csv\")\n",
        "goodreads_test.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf_vj_5uAnm5",
        "outputId": "f464645b-7b20-4e06-e11c-af00ed6ce959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200000 entries, 0 to 199999\n",
            "Data columns (total 11 columns):\n",
            " #   Column        Non-Null Count   Dtype \n",
            "---  ------        --------------   ----- \n",
            " 0   Unnamed: 0    200000 non-null  int64 \n",
            " 1   user_id       200000 non-null  object\n",
            " 2   book_id       200000 non-null  int64 \n",
            " 3   review_id     200000 non-null  object\n",
            " 4   review_text   200000 non-null  object\n",
            " 5   date_added    200000 non-null  object\n",
            " 6   date_updated  200000 non-null  object\n",
            " 7   read_at       179646 non-null  object\n",
            " 8   started_at    139114 non-null  object\n",
            " 9   n_votes       200000 non-null  int64 \n",
            " 10  n_comments    200000 non-null  int64 \n",
            "dtypes: int64(4), object(7)\n",
            "memory usage: 16.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "goodreads_train.columns"
      ],
      "metadata": {
        "id": "v-2GIeyLDq3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239f1001-43fa-45bb-8eae-0c936c1e590e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['user_id', 'book_id', 'review_id', 'rating', 'review_text',\n",
              "       'date_added', 'date_updated', 'read_at', 'started_at', 'n_votes',\n",
              "       'n_comments'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = goodreads_train.drop([\"user_id\", \"book_id\", \"review_id\", \"date_added\", \"date_updated\", \"read_at\",\"started_at\"], axis=1)\n",
        "test_data = goodreads_test.drop([\"user_id\", \"book_id\", \"review_id\", \"date_added\", \"date_updated\", \"read_at\",\"started_at\"], axis=1)"
      ],
      "metadata": {
        "id": "DJ2IyUh2EAEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data.dropna()\n",
        "test_data.head()"
      ],
      "metadata": {
        "id": "Sw8iU_f2E_9h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "b24cb402-f7b9-43c3-e222-acc5cf83444a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                        review_text  n_votes  \\\n",
              "0        2212  3-3.5 stars \\n While I liked it, I didn't love...        1   \n",
              "1      655187  First, I must fangirl over the ending. *OMG FA...        0   \n",
              "2      483724  Somewhere between 4 and 5 - really loved this ...        1   \n",
              "3      322858  I'll admit it - I was not expecting to enjoy t...        0   \n",
              "4      393932  Book #25 of 2011 \\n I didn't read the accompyi...        0   \n",
              "\n",
              "   n_comments  \n",
              "0           0  \n",
              "1           0  \n",
              "2           0  \n",
              "3           0  \n",
              "4           0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-835147c5-fb64-4f2f-b939-4136ca8c8a35\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review_text</th>\n",
              "      <th>n_votes</th>\n",
              "      <th>n_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2212</td>\n",
              "      <td>3-3.5 stars \\n While I liked it, I didn't love...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>655187</td>\n",
              "      <td>First, I must fangirl over the ending. *OMG FA...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>483724</td>\n",
              "      <td>Somewhere between 4 and 5 - really loved this ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>322858</td>\n",
              "      <td>I'll admit it - I was not expecting to enjoy t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>393932</td>\n",
              "      <td>Book #25 of 2011 \\n I didn't read the accompyi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-835147c5-fb64-4f2f-b939-4136ca8c8a35')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-835147c5-fb64-4f2f-b939-4136ca8c8a35 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-835147c5-fb64-4f2f-b939-4136ca8c8a35');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.dropna()\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "Ui-EwatlFS6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "eaeecdf9-a8bb-4cd9-94e6-784bf79b91e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   rating                                        review_text  n_votes  \\\n",
              "0       5  I thought this was a very good historical fant...        5   \n",
              "1       5  I've had this book recommended to me more than...        0   \n",
              "2       5  Seriously! You go and throw that twist into th...        0   \n",
              "3       4  Gosh!!! I'm still not recovering from the effi...        5   \n",
              "4       3  3.5 stars rounded down. I can't help it, I jus...        0   \n",
              "\n",
              "   n_comments  \n",
              "0           6  \n",
              "1           0  \n",
              "2           0  \n",
              "3           0  \n",
              "4           0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48e73d21-fcfe-483e-83b6-93ead81a0253\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>review_text</th>\n",
              "      <th>n_votes</th>\n",
              "      <th>n_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>I thought this was a very good historical fant...</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>I've had this book recommended to me more than...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>Seriously! You go and throw that twist into th...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Gosh!!! I'm still not recovering from the effi...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>3.5 stars rounded down. I can't help it, I jus...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48e73d21-fcfe-483e-83b6-93ead81a0253')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48e73d21-fcfe-483e-83b6-93ead81a0253 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48e73d21-fcfe-483e-83b6-93ead81a0253');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the text data\n",
        "\n"
      ],
      "metadata": {
        "id": "9t8kSy_DlU79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase the review text\n",
        "train_review = train_data[\"review_text\"].apply(lambda x: str(x).lower())\n",
        "test_review = test_data[\"review_text\"].apply(lambda x: str(x).lower())"
      ],
      "metadata": {
        "id": "v8KFbOUPkbax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the line breaks and extra spaces\n",
        "train_review = train_review.apply(lambda x: \" \".join(x.split()))\n",
        "test_review = test_review.apply(lambda x: \" \".join(x.split()))"
      ],
      "metadata": {
        "id": "w3RDR_88lkIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Removing https links from the text\n",
        "train_review = train_review.apply(\n",
        "    lambda x: re.sub(\n",
        "        r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '',\n",
        "        x, flags=re.MULTILINE))\n",
        "\n",
        "test_review = test_review.apply(\n",
        "    lambda x: re.sub(\n",
        "        r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '',\n",
        "        x, flags=re.MULTILINE))"
      ],
      "metadata": {
        "id": "gEwTRItumPZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special charaters from the review text\n",
        "train_review = train_review.apply(lambda x: re.sub('\\W+',' ', x))\n",
        "test_review = test_review.apply(lambda x: re.sub('\\W+',' ', x))"
      ],
      "metadata": {
        "id": "oiiq06Uumurp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk and download stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the stop words\n",
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "# View the count of stop_words\n",
        "len(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzAhXQxonD8p",
        "outputId": "7b60a77f-2238-4b33-c480-5d4d8ffe2e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop_words from reviews\n",
        "train_review = train_review.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "test_review = test_review.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
      ],
      "metadata": {
        "id": "vrnFaOjfnGbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove repeated words from reviews\n",
        "train_review = train_review.apply(lambda x: ' '.join(set(x.split())))\n",
        "test_review = test_review.apply(lambda x: ' '.join(set(x.split())))"
      ],
      "metadata": {
        "id": "HB3rjGJAneM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Model\n",
        "\n"
      ],
      "metadata": {
        "id": "MSBGKK1moN1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two baseline and one deep NN models"
      ],
      "metadata": {
        "id": "PWOMt15_jUh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "id": "KJ5rAm4ZuRW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding layer\n",
        "\n"
      ],
      "metadata": {
        "id": "yCunOeBqPRzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training data"
      ],
      "metadata": {
        "id": "tiNP-_oRseLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/goodread-class-project/my_goodreads_train.csv',usecols=['review_text','rating'],nrows=300000)\n",
        "data.head(3)"
      ],
      "metadata": {
        "id": "XgoOwuo3ubwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the sentence\n",
        "sentence = data['review_text']\n",
        "\n",
        "# Define the pre-trained GloVe embeddings\n",
        "embedding_dim = 50\n",
        "# glove_file = \"path/to/glove.6B.100d.txt\"\n",
        "# glove_file = \"/content/drive/MyDrive/Goodread/glove.6B.100d.txt\"\n",
        "glove_file = \"/content/drive/MyDrive/goodread-class-project/glove.6B.50d.txt\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts([sentence])\n",
        "tokenizer.fit_on_texts(data['review_text'])\n",
        "word_index = tokenizer.word_index\n",
        "# sequences = tokenizer.texts_to_sequences([sentence])\n",
        "sequences = tokenizer.texts_to_sequences(data['review_text'])\n",
        "\n",
        "# Pad the sequence to a fixed length\n",
        "# max_length = 10\n",
        "max_length = 50\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Load the pre-trained GloVe embeddings\n",
        "embeddings_index = {}\n",
        "with open(glove_file) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(len(word_index) + 1,\n",
        "                                            embedding_dim,\n",
        "                                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                                            trainable=False)\n",
        "\n",
        "# Pass the padded sequence through the embedding layer\n",
        "embedded_sequence = embedding_layer(padded_sequences)\n",
        "\n",
        "# Flatten the embedding to a 1D vector\n",
        "embedded_vector = tf.keras.layers.Flatten()(embedded_sequence)\n",
        "\n",
        "# Convert the embedded vector to a numpy array\n",
        "embedded_array = embedded_vector.numpy()\n",
        "\n",
        "# print(embedded_array)\n"
      ],
      "metadata": {
        "id": "tm_LhPSZ8Qvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing data"
      ],
      "metadata": {
        "id": "m40iDIOEskye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_csv('/content/drive/MyDrive/goodread-class-project/my_goodreads_test.csv',usecols=['review_text'])\n",
        "data_test.head(3)"
      ],
      "metadata": {
        "id": "vEYHR9dKuaST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the sentence\n",
        "sentence = data_test['review_text']\n",
        "\n",
        "# Define the pre-trained GloVe embeddings\n",
        "embedding_dim = 50\n",
        "# glove_file = \"path/to/glove.6B.100d.txt\"\n",
        "# glove_file = \"/content/drive/MyDrive/Goodread/glove.6B.100d.txt\"\n",
        "glove_file = \"/content/drive/MyDrive/goodread-class-project/glove.6B.50d.txt\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts([sentence])\n",
        "tokenizer.fit_on_texts(data_test['review_text'])\n",
        "word_index = tokenizer.word_index\n",
        "# sequences = tokenizer.texts_to_sequences([sentence])\n",
        "sequences = tokenizer.texts_to_sequences(data_test['review_text'])\n",
        "\n",
        "# Pad the sequence to a fixed length\n",
        "# max_length = 10\n",
        "max_length = 50\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# Load the pre-trained GloVe embeddings\n",
        "embeddings_index = {}\n",
        "with open(glove_file) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "embedding_layer = tf.keras.layers.Embedding(len(word_index) + 1,\n",
        "                                            embedding_dim,\n",
        "                                            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                                            trainable=False)\n",
        "\n",
        "# Pass the padded sequence through the embedding layer\n",
        "embedded_sequence = embedding_layer(padded_sequences)\n",
        "\n",
        "# Flatten the embedding to a 1D vector\n",
        "embedded_vector = tf.keras.layers.Flatten()(embedded_sequence)\n",
        "\n",
        "# Convert the embedded vector to a numpy array\n",
        "embedded_array = embedded_vector.numpy()\n",
        "\n",
        "# print(embedded_array)"
      ],
      "metadata": {
        "id": "Pd3FS2dw-N39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.DataFrame(embedded_array)\n",
        "\n",
        "X = df.drop(['rating'],axis=1)\n",
        "y = df['rating']\n",
        "\n",
        "y = pd.get_dummies(y)"
      ],
      "metadata": {
        "id": "VdMXHSmXtaA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "L8W9t4KYtitw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First NN Model\n"
      ],
      "metadata": {
        "id": "HdiRscd_uysD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(2500,)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "\n",
        "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7Z6O49Xs9OTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=100, batch_size=256, callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "Ec6LN0dHu7nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second NN Model"
      ],
      "metadata": {
        "id": "g94LbGcIvHqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = Sequential()\n",
        "\n",
        "# Adding the first hidden layer with 256 units, using the relu activation function and L2 regularization\n",
        "model.add(Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_dim=2500))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding the second hidden layer with 128 units, using the relu activation function and L2 regularization\n",
        "model.add(Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding the second hidden layer with 128 units, using the relu activation function and L2 regularization\n",
        "model.add(Dense(units=128, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding the output layer with 5 units and softmax activation function\n",
        "model.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Defining early stopping to prevent overfitting\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "\n",
        "# Fitting the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "# Evaluating the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "xtRWNMBg9OLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Third NN Model"
      ],
      "metadata": {
        "id": "Bnog70StwSpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "import numpy as np\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "def lr_scheduler(epoch):\n",
        "    lr = 0.001\n",
        "    if epoch > 50:\n",
        "        lr *= np.exp(-0.1)\n",
        "    return lr\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add the first hidden layer with 1024 units, using the tanh activation function and L2 regularization\n",
        "model.add(Dense(units=1024, activation='tanh', kernel_regularizer=regularizers.l2(0.01), input_dim=2500))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the second hidden layer with 512 units, using the sigmoid activation function and dropout regularization\n",
        "model.add(Dense(units=512, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the third hidden layer with 256 units, using the relu activation function and batch normalization\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add the fourth hidden layer with 128 units, using the tanh activation function and L2 regularization\n",
        "model.add(Dense(units=128, activation='tanh', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add the output layer with 5 units and softmax activation function\n",
        "model.add(Dense(units=5, activation='softmax'))\n",
        "\n",
        "# Compile the model with the RMSprop optimizer and learning rate scheduler\n",
        "opt = RMSprop(lr=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping and learning rate scheduler callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "lr_scheduler = LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stop, lr_scheduler])\n"
      ],
      "metadata": {
        "id": "FiGhYsrA9OJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM\n"
      ],
      "metadata": {
        "id": "aqbjz_r4grEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation\n",
        "\n"
      ],
      "metadata": {
        "id": "kDS3z1v0kl4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data[\"review_text\"])\n",
        "\n",
        "x_train_seq = tokenizer.texts_to_sequences(train_data[\"review_text\"])\n",
        "x_test_seq = tokenizer.texts_to_sequences(test_data[\"review_text\"])"
      ],
      "metadata": {
        "id": "OvKDVRAuF6AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "x_train_pad = pad_sequences(x_train_seq, maxlen=100, padding='post')\n",
        "x_train_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOsLyq1wlGig",
        "outputId": "821c9221-9e10-4fad-bcfe-4d2d62ddec13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  17,    5,  523, ...,   49, 1000,   24],\n",
              "       [ 159,   55,   12, ...,    0,    0,    0],\n",
              "       [ 441,   22,  148, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [ 620,  396,  223, ...,    0,    0,    0],\n",
              "       [3629,   61,    1, ...,   15,    1,  177],\n",
              "       [9354,    4,  154, ...,  259,  360, 2060]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_pad = pad_sequences(x_test_seq, maxlen=100, padding='post', truncating='post')\n",
        "x_test_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWcxca1-C_O7",
        "outputId": "6d2d780c-9e4c-4de8-e5ca-9950942deb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 191,  191,  136, ...,    2,    1,  517],\n",
              "       [  66,    3,  360, ...,   13, 3409,  298],\n",
              "       [1285,  146,  223, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [3057,   30,   29, ...,    0,    0,    0],\n",
              "       [   3,   36,  121, ...,    0,    0,    0],\n",
              "       [  12,    9,    5, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_prepared = pd.DataFrame(x_train_pad)\n",
        "# Because the output labels are one-hot vectors, so I also have to convert the lables to one-hot vectors before training the model.\n",
        "from keras.utils import to_categorical\n",
        "y_prepared = to_categorical(train_data[\"rating\"]-1, num_classes=5)"
      ],
      "metadata": {
        "id": "b2j5juxmPCK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_mytest, y_train, y_mytest = train_test_split(x_prepared, y_prepared, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "CI-op1rqnj9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model\n",
        "\n"
      ],
      "metadata": {
        "id": "0cT7BE9wkvTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras import optimizers\n",
        "from keras.regularizers import l2\n",
        "vocabulary_size = len(tokenizer.word_counts.keys())+1\n",
        "max_words = 100\n",
        "embedding_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
        "# model.add(LSTM(10, return_sequences=True))\n",
        "model.add(LSTM(128, dropout=0.7, recurrent_dropout=0.7))\n",
        "# model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.7))\n",
        "# model.add(LSTM(128, dropout=0.7, recurrent_dropout=0.7))\n",
        "model.add(Dense(5, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "w-P9GFfbl_oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using adam as the optimizer and setting 0.0001 as the learning rate.\n",
        "optimizer = optimizers.Adam(learning_rate = 0.001)"
      ],
      "metadata": {
        "id": "WS9gGHjToazz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer= optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "zn_N6pnyCTJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "2l2ir05mDKSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b872a4ae-2c1e-4a8a-f849-3830cfdffbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          31429200  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               117248    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,547,093\n",
            "Trainable params: 31,547,093\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the early stopping callback\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights = True)\n"
      ],
      "metadata": {
        "id": "dJze1y5CWZf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, shuffle =True, callbacks=[early_stopping])\n",
        "result = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, shuffle =True)"
      ],
      "metadata": {
        "id": "6_x4ArXu_rFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beec7191-0419-44e3-88d7-3b613a8d7752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            " 6363/14000 [============>.................] - ETA: 2:24:25 - loss: 1.3592 - accuracy: 0.3764"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert\n"
      ],
      "metadata": {
        "id": "CqmCp__fg00I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ily8yhyilCdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_train = tokenizer.batch_encode_plus(train_review, padding=True, truncation=True, return_attention_mask=True, return_token_type_ids=True, return_tensors='pt')\n",
        "tokenized_test = tokenizer.batch_encode_plus(test_review, padding=True, truncation=True, return_attention_mask=True, return_token_type_ids=True, return_tensors='pt')\n"
      ],
      "metadata": {
        "id": "cPfUjfQVlFcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "labels = to_categorical(train_data[\"rating\"]-1, num_classes=5)\n",
        "\n",
        "import torch\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train_input_ids, val_input_ids, train_labels, val_labels = train_test_split(tokenized_train, labels, test_size=0.2, random_state=42)\n",
        "train_input_ids, val_input_ids, train_attention_masks, val_attention_masks, train_labels, val_labels = train_test_split(tokenized_train['input_ids'], tokenized_train['attention_mask'], labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "tiN_EupplQfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VcvlPQwsk1Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "bert_model = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the classification layer\n",
        "classification_layer = tf.keras.layers.Dense(5, activation='softmax')\n",
        "\n",
        "# Define the model architecture\n",
        "input_ids = tf.keras.Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
        "bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]\n",
        "classification_output = classification_layer(bert_output)\n",
        "model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=classification_output)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Convert PyTorch tensors to TensorFlow tensors\n",
        "train_input_ids = tf.convert_to_tensor(train_input_ids.numpy())\n",
        "val_input_ids = tf.convert_to_tensor(val_input_ids.numpy())\n",
        "train_attention_masks = tf.convert_to_tensor(train_attention_masks.numpy())\n",
        "val_attention_masks = tf.convert_to_tensor(val_attention_masks.numpy())\n",
        "\n",
        "train_labels = tf.convert_to_tensor(train_labels.numpy())\n",
        "val_labels = tf.convert_to_tensor(val_labels.numpy())\n",
        "\n",
        " model.fit(\n",
        "    x=[train_input_ids, train_attention_masks], y=train_labels,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    validation_data=([val_input_ids, val_attention_masks], val_labels)\n",
        ")"
      ],
      "metadata": {
        "id": "zWZyd6nIhEw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result"
      ],
      "metadata": {
        "id": "cfHUCGhKhBzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_mytest, y_mytest)\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "LqWHu3_qC7ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_id = goodreads_test['review_id']\n",
        "result = model.predict(x_test_pad)\n",
        "# pre_result = pd.DataFrame(data=result, columns=[ \"rating\"], index=review_id)\n",
        "# pre_result.to_csv('prediction.csv')"
      ],
      "metadata": {
        "id": "rVnbCNU6Deiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predicted_classes = np.argmax(result, axis=-1) + 1\n",
        "predicted_classes"
      ],
      "metadata": {
        "id": "NKfm3CoCQ3GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_result = pd.DataFrame(data=predicted_classes, columns=[ \"rating\"], index=review_id)\n",
        "review_id = pd.DataFrame(data=review_id, columns=[ \"review_id\"])\n",
        "pre_result.join(other=review_id, lsuffix='_')\n",
        "\n",
        "pre_result.to_csv('prediction.csv')"
      ],
      "metadata": {
        "id": "-wzkgvYOw00B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}